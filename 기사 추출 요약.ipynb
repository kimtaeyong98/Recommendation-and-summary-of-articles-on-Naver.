{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "class NewsCrawler(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            press_url: str,\n",
    "    ) -> None:\n",
    "        self.press_url = press_url\n",
    "\n",
    "    def __call__(self):\n",
    "        news_list = list()\n",
    "        title_list = list()\n",
    "        section_list = list()\n",
    "        \n",
    "        url_list = self._url_crawling()\n",
    "        for url in url_list:\n",
    "            news, section, title = self._news_crawling(url)\n",
    "            if news is not None and news[:2] != 'if' and len(news) > 150:\n",
    "                news_list.append(news)\n",
    "                section_list.append(section)\n",
    "                title_list.append(title)\n",
    "\n",
    "        return news_list, title_list, section_list\n",
    "\n",
    "    def _url_crawling(self) -> list:\n",
    "        url_list = list()\n",
    "        base_url = 'https://news.naver.com'\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        \n",
    "        html = requests.get(self.press_url, headers=headers)\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "        news = soup.find('ul', class_='rankingnews_list type_detail')\n",
    "\n",
    "        urls = news.find_all('a')\n",
    "\n",
    "        for url in urls:\n",
    "            href = url.attrs['href']\n",
    "            if href is not None:\n",
    "                url_list.append(base_url + href)\n",
    "\n",
    "        return url_list\n",
    "\n",
    "    def _news_crawling(self, url: str):\n",
    "        def _clean_text(text):\n",
    "            text = re.sub('[a-zA-Z]', \"\", text)\n",
    "            text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>◀◁▷▶■♥⑤◆※@\\#$%&\\\\\\=\\(\\\"\\\"(\\n)(\\t)]', \"\", text)\n",
    "            text = text.replace(\"오류를 우회하기 위한 함수 추가\", \"\")\n",
    "            text = text.replace(\"사진연합뉴스\", \"\")\n",
    "            text = text.replace(\"무단 전재 및 재배포 금지\", \"\")\n",
    "            text = text.replace(\"동영상 뉴스\", \"\")\n",
    "            cleaned_text = text.replace(\"앵커\", \"\")\n",
    "\n",
    "            return cleaned_text\n",
    "    \n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "        html = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "        all_news = soup.find(id='articleBodyContents').text\n",
    "        all_news = _clean_text(all_news).strip()\n",
    "        \n",
    "        title = soup.find(id='articleTitle').text.strip()\n",
    "        section = soup.find('em', class_='guide_categorization_item').text.strip()\n",
    "\n",
    "        return all_news, section, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class NewsCollector(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            press_name: str,\n",
    "    ) -> None:\n",
    "        press = {'KBS': '056', 'MBC': '214', 'SBS': '055', 'JTBC': '437', 'YTN': '052', '한국경제TV': '215',\n",
    "                 'TV조선': '448', '연합뉴스': '001', '연합뉴스TV': '422', '뉴스1': '421', '뉴시스': '003',\n",
    "                 '중앙일보': '025', '동아일보': '020', '매일경제': '009', '전자신문': '030', '이데일리': '018',\n",
    "                 '서울경제' : '011', '머니투데이': '008', '채널A': '449'}\n",
    "\n",
    "        if press_name in self.available_presses():\n",
    "            self.press_name = press_name\n",
    "        else:\n",
    "            raise KeyError('Unknown press : {}, available presses are {}'.format(press_name, self.available_presses()))\n",
    "\n",
    "        self.base_url = 'https://news.naver.com/main/ranking/office.nhn?officeId='\n",
    "        self.press_url = self.base_url + press[self.press_name]\n",
    "\n",
    "    def collect(self) -> list:\n",
    "        collector = NewsCrawler(self.press_url)\n",
    "        news_list, title_list, section_list = collector()\n",
    "        \n",
    "        arr = list(set(zip(title_list, section_list, news_list)))\n",
    "        \n",
    "        return arr\n",
    "    @staticmethod\n",
    "    def available_presses() -> list:\n",
    "        return ['KBS', 'MBC', 'SBS', 'JTBC', 'YTN', '한국경제TV', \n",
    "                'TV조선', '연합뉴스', '연합뉴스TV', '뉴스1', '뉴시스',\n",
    "                '중앙일보', '동아일보', '매일경제', '전자신문', '이데일리', \n",
    "                '서울경제', '머니투데이', '채널A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dic = {'Title': list(), 'Section': list(), 'News': list()}\n",
    "for press in NewsCollector.available_presses():\n",
    "    news_collector = NewsCollector(press)\n",
    "    try:\n",
    "        arr = news_collector.collect()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        dic['Title'].append(arr[i][0])\n",
    "        dic['Section'].append(arr[i][1])\n",
    "        dic['News'].append(arr[i][2])\n",
    "        \n",
    "df = pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "fp = open(\"stopwords.txt\", \"r\", encoding = 'UTF8')\n",
    "stopwords = fp.read().split()\n",
    "\n",
    "def get_nouns(x):\n",
    "    nouns = okt.nouns(x)#명사추출\n",
    "    nouns = [noun for noun in nouns if len(noun) > 1]#한글자 제거\n",
    "    nouns = [noun for noun in nouns if noun not in stopwords]#불용어 처리\n",
    "\n",
    "    return nouns\n",
    "\n",
    "def text_cleaning(text):#한글만 남기기\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "    result = hangul.sub('', text)\n",
    "    return result\n",
    "\n",
    "def start(df,section):\n",
    "    df2=df.copy()\n",
    "    df2=df2[df2['Section'] == section]#색션 같은 기사만 추출\n",
    "    df2['News']=df2['Title']+df2['News']# 타이틀이랑 합치기\n",
    "    df2['only_hangul'] = df2['News'].apply(lambda x : text_cleaning(x))#한글만 남기기\n",
    "    df2['nouns']=df2['only_hangul'].apply(lambda x: get_nouns(x))#명사만 추출\n",
    "    return df2\n",
    "\n",
    "df2 = start(df,'사회')\n",
    "data_split = df2['News'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic class\n",
    "class topic(object):\n",
    "    def get_freq(self, data):\n",
    "        pocket_size = 20\n",
    "        noun = []\n",
    "        noun_proto = okt.nouns(data)\n",
    "        \n",
    "        for words in noun_proto:\n",
    "            if (len(words)>1) and (words not in stop_words):\n",
    "                noun.append(words)\n",
    "                \n",
    "        count = Counter(noun)        \n",
    "        noun_list = count.most_common(pocket_size)\n",
    "    \n",
    "        if(len(noun_list) < pocket_size):\n",
    "            pocket_size = len(noun_list)\n",
    "        \n",
    "        return noun_list\n",
    "    \n",
    "    def get_topic(self, data, amount):\n",
    "        doc = []\n",
    "        for article in data:\n",
    "            freq_list = self.get_freq(article)\n",
    "            \n",
    "            for freq in freq_list:\n",
    "                doc.append((freq[0]))\n",
    "            \n",
    "        total = Counter(doc)\n",
    "    \n",
    "        return total.most_common(amount)\n",
    "    \n",
    "    # 특정 keyword 에 대한 연관단어 추출\n",
    "    def get_topic_include(self, data, amount, keyword):\n",
    "        doc = []\n",
    "        for article in data:\n",
    "            if(okt.nouns(article).count(keyword) == 0):continue\n",
    "                \n",
    "            freq_list = self.get_freq(article)\n",
    "\n",
    "            for freq in freq_list:\n",
    "                doc.append((freq[0]))\n",
    "            \n",
    "        total = Counter(doc)\n",
    "    \n",
    "        return total.most_common(amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight calculater\n",
    "class calc(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def cal_weight(self, includes):\n",
    "        freq_sum = 0\n",
    "        result_sum = 0\n",
    "        result_list = []\n",
    "        for words in includes: #핵심단어로 지정된 단어들이 등장한 문서의 합\n",
    "            freq_sum+= words[1]\n",
    "\n",
    "        for article in self.data:\n",
    "            result_sum = 0\n",
    "            for words in includes:\n",
    "                result_sum += words[1] * (okt.nouns(article).count(words[0])/len(article))\n",
    "                #result_sum += (words[1]/freq_sum) * (okt.nouns(article).count(words[0])/len(article)) 잠시변경\n",
    "            result_list.append(result_sum)\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result\n",
    "def result_generator(selec):\n",
    "    result_list = list()\n",
    "    max_ = 0\n",
    "    index_ = 0\n",
    "    include = topics.get_topic_include(data_split, 10, main_topic[selec][0])\n",
    "    calculater = calc(data_split)\n",
    "    result = calculater.cal_weight(include)\n",
    "    i = 1\n",
    "    print(include[0][0],\"에 대해서\")\n",
    "    print(include)\n",
    "    for value in result:\n",
    "        print(i,\"번째기사 =\", value)\n",
    "        result_list.append(value)\n",
    "        if(value > max_):\n",
    "            max_ = value\n",
    "            index_ = i\n",
    "        i+=1\n",
    "    print(\"max:\", index_,\"번째기사 =\", max_)\n",
    "    return result_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main frame 1 - 데이터 로드 및 선언\n",
    "doc = []\n",
    "topics = topic()\n",
    "stop_words = stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main frame 2 - 전체 데이터(기사)의 토픽 > 핫이슈\n",
    "main_topic = topics.get_topic(data_split, 5)\n",
    "print(main_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#main frame 3 - 핫이슈 별 연관 키워드(해당 단어를 포함한 기사들만 모아서 재분석)\n",
    "for issue in main_topic:\n",
    "    include = topics.get_topic_include(data_split, 10, issue[0])\n",
    "    print(\"<Topic:\", issue[0],\">\")\n",
    "    print(include, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1_사용자 입력\n",
    "status = True\n",
    "j = 0\n",
    "choice = \"hello\"\n",
    "for words in main_topic:\n",
    "    print((j+1),\":\", words[0], sep ='', end = ' ')\n",
    "    j += 1\n",
    "\n",
    "print(\"\\n입력:\",end = '')\n",
    "sel = int(input())-1\n",
    "rl = result_generator(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(target):\n",
    "    normalized_list = []\n",
    "    \n",
    "    for value in target:\n",
    "        normalized_target = (value - min(target)) / (max(target) - min(target))\n",
    "        normalized_list.append(normalized_target)\n",
    "    \n",
    "    return normalized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "def Apriori(df2):#Apriori알고리즘\n",
    "    Threshold=0.2\n",
    "    transactions = df2['nouns'].tolist()\n",
    "    transactions = [transaction for transaction in transactions if transaction] # 공백 문자열을 방지\n",
    "    \n",
    "    results = list(apriori(transactions,min_support=0.06,\n",
    "    min_confidence=0.05,\n",
    "    min_lift=1.0,\n",
    "    max_length=2))\n",
    "    \n",
    "    columns = ['source', 'target', 'support']\n",
    "    network_df2 = pd.DataFrame(columns=columns)\n",
    "    for result in results:\n",
    "        if len(result.items) == 2 and result.support>=Threshold:\n",
    "            items = [x for x in result.items]\n",
    "            row = [items[0], items[1], result.support]\n",
    "            series = pd.Series(row, index=network_df2.columns)\n",
    "            network_df2 = network_df2.append(series, ignore_index=True)\n",
    "\n",
    "    network_df2=network_df2.sort_values(by='support', ascending=False)\n",
    "    network_df=pd.DataFrame(results)\n",
    "    network_df['length']=network_df['items'].apply(lambda x: len(x))\n",
    "    \n",
    "    network_df=network_df[(network_df['length']==2) & \n",
    "                          (network_df['support']>=Threshold)].sort_values(by='support', ascending=False)\n",
    "    \n",
    "    network_df=network_df.drop(columns=['ordered_statistics'])\n",
    "    \n",
    "    network_df=network_df.reset_index()\n",
    "    network_df2=network_df2.reset_index()\n",
    "    \n",
    "    network_df['source']=network_df2['source']\n",
    "    network_df['target']=network_df2['target']\n",
    "    network_df=network_df.drop(columns=['index'])\n",
    "    return network_df\n",
    "\n",
    "network_df = Apriori(df2)\n",
    "topic_userinput=main_topic[sel][0]\n",
    "df2 = df2.reset_index(drop = True)\n",
    "\n",
    "network_df2=network_df[network_df['source']==topic_userinput]\n",
    "topic_about=network_df2['target'].tolist()\n",
    "topic_about.append(topic_userinput)\n",
    "\n",
    "count_list=[]\n",
    "df3 = df2.copy()\n",
    "for i in df3['nouns']:#1행씩\n",
    "    count=0\n",
    "    for j in range(len(i)):#각행의 단어들\n",
    "        for k in topic_about:#각행의 단어와 키워드 비교\n",
    "            if i[j] ==k:\n",
    "                count+=1\n",
    "    count_list.append(count)\n",
    "#두 값을 동일하게 반영하기 위해서 min-max 정규화를 통해 0~1사이값으로 바꿈\n",
    "df3[\"value\"] = min_max_normalize(rl)\n",
    "df3['count']= min_max_normalize(count_list)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기사추출\n",
    "df3['add']= (df3['value']+df3['count'])/2 #두 지표의 평균\n",
    "#df3['mul']= df3['value']*df3['count'] #두 지표의 곱\n",
    "df_result = df3.sort_values(by='add', ascending=False)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#요약하기-카카오브레인 pororo모델\n",
    "from pororo import Pororo\n",
    "summary=Pororo(task=\"summary\", lang=\"kr\")\n",
    "result=summary(df_result[\"NEWS\"][0])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화 -참고\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib import rc\n",
    "font_name = fm.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "def Visualization(network_df):\n",
    "    G = nx.Graph()\n",
    "    ar=(network_df['items'])\n",
    "    G.add_edges_from(ar)\n",
    "    pr=nx.pagerank(G)\n",
    "    nsize=np.array([v for v in pr.values()])\n",
    "    nsize= 2000* (nsize-min(nsize)) / (max(nsize)-min(nsize))\n",
    "    \n",
    "    pos=nx.random_layout(G)\n",
    "    \n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.axis('off')\n",
    "    nx.draw_networkx(G, font_family=font_name, font_size=16,\n",
    "                    pos=pos, node_color=list(pr.values()),node_size=nsize,\n",
    "                    alpha=0.7, edge_color='.5')\n",
    "    #nx.draw_networkx_labels(G, pos, font_family=font_name, font_size=15)\n",
    "    plt.show()\n",
    "Visualization(network_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
